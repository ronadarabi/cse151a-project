{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet Training\n",
    "\n",
    "## Important note:\n",
    "**Multithreading does not work in jupyter notebooks, so its disabled here. This file is meant as an overview of the training process, it works but since theres no concurrency it takes aproximately 4 times as long to train**\n",
    "\n",
    "This notebook can be used to follow the training steps of our convnet model, this was our first prototype model and doesnt have many optimizations, and works RGB colorspace instead of lab. Because of this, the model isn't great, but was an important first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderRGBDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg')):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        gray_image = transforms.functional.to_grayscale(image, num_output_channels=1)\n",
    "        gray_image = torch.tensor(np.array(gray_image)).unsqueeze(0).float() / 255.0\n",
    "        rgb_image = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return gray_image, rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSettings:\n",
    "    root_dir: str = \"img_data\"\n",
    "    validation_image_count: int = 1024\n",
    "    batch_size: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    validation_steps: int = 1000\n",
    "    learning_rate: float = 0.0007 # Peak LR\n",
    "    min_lr: float = learning_rate / 10  # Minimum LR\n",
    "    weight_decay: float = 1e-5\n",
    "    warmup_steps: int = 1000 # Linear warmup over n steps\n",
    "    validation_steps: int = 1000 # Validate every n steps\n",
    "    loss_function: str = \"HuberLoss\"\n",
    "    optimizer: str = \"Adam\"\n",
    "    model_name: str = \"ConvNet\"\n",
    "\n",
    "    def set_total_image_count(self, count):\n",
    "        self.total_image_count = count\n",
    "        self.num_steps = (self.total_image_count - self.validation_image_count) // self.batch_size\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.total_image_count = None\n",
    "        self.num_steps = None\n",
    "\n",
    "    def create_run_name(self):\n",
    "        return f\"{self.model_name}_lr{self.learning_rate}_bs{self.batch_size}_steps{self.num_steps}_loss{self.loss_function}_opt{self.optimizer}\"\n",
    "\n",
    "\n",
    "settings = ModelSettings()\n",
    "dataset = ImageFolderRGBDataset(root_dir=settings.root_dir)\n",
    "settings.set_total_image_count(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "We used a custom learing rate schedule, a linear warmup followed by cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step: int):\n",
    "    if current_step < settings.warmup_steps:\n",
    "        return float(current_step) / float(max(1, settings.warmup_steps))\n",
    "    cosine_decay = 0.5 * (\n",
    "        1\n",
    "        + np.cos(\n",
    "            np.pi\n",
    "            * (current_step - settings.warmup_steps)\n",
    "            / (settings.num_steps - settings.warmup_steps)\n",
    "        )\n",
    "    )\n",
    "    return (\n",
    "        settings.min_lr + (settings.learning_rate - settings.min_lr) * cosine_decay\n",
    "    ) / settings.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, test_dataloader, criterion):\n",
    "    val_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_images = 0\n",
    "    logged_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (L, RGB) in enumerate(test_dataloader):\n",
    "            L, RGB = L.to(device), RGB.to(device)\n",
    "\n",
    "            outputs = model(L)\n",
    "            loss = criterion(outputs, RGB)\n",
    "            val_loss += loss.item()\n",
    "            total_images += L.size(0)\n",
    "\n",
    "            if logged_images < 8:\n",
    "                num_samples = min(8 - logged_images, L.shape[0])\n",
    "                L_samples = L[:num_samples].cpu().numpy()\n",
    "                output_samples = outputs[:num_samples].cpu().numpy()\n",
    "                target_samples = RGB[:num_samples].cpu().numpy()\n",
    "\n",
    "                L_rgb_samples = [\n",
    "                    np.repeat(L_samples[j], 3, axis=0).transpose(1, 2, 0)\n",
    "                    for j in range(num_samples)\n",
    "                ]\n",
    "                output_rgb_samples = [\n",
    "                    output_samples[j].transpose(1, 2, 0)\n",
    "                    for j in range(num_samples)\n",
    "                ]\n",
    "                target_rgb_samples = [\n",
    "                    target_samples[j].transpose(1, 2, 0)\n",
    "                    for j in range(num_samples)\n",
    "                ]\n",
    "\n",
    "                stacked_L_rgb = np.hstack(L_rgb_samples)\n",
    "                stacked_output_rgb = np.hstack(output_rgb_samples)\n",
    "                stacked_target_rgb = np.hstack(target_rgb_samples)\n",
    "\n",
    "                stacked_images = np.vstack(\n",
    "                    (stacked_L_rgb, stacked_output_rgb, stacked_target_rgb)\n",
    "                )\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"Examples\": wandb.Image(\n",
    "                            stacked_images,\n",
    "                            caption=\"Top: Grayscale, Middle: Predicted, Bottom: True\",\n",
    "                        )\n",
    "                    }, commit=False\n",
    "                )\n",
    "                \n",
    "                logged_images += num_samples\n",
    "\n",
    "    avg_val_loss = val_loss / (total_images / settings.batch_size)\n",
    "    val_time = time.time() - val_start_time\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}, Validation Time: {val_time:.4f}s\")\n",
    "\n",
    "    wandb.log({\"Average Validation Loss\": avg_val_loss, \"Validation Time\": val_time}, commit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, num_steps, validation_steps):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step_times = []\n",
    "\n",
    "    train_iter = iter(train_dataloader)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        try:\n",
    "            L, RGB = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_dataloader)\n",
    "            L, RGB = next(train_iter)\n",
    "\n",
    "        L, RGB = L.to(device), RGB.to(device)\n",
    "\n",
    "        outputs = model(L)\n",
    "        loss = criterion(outputs, RGB)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        step_end_time = time.time()\n",
    "        step_times.append(step_end_time)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step > 0:\n",
    "            step_time = step_times[-1] - step_times[-2]\n",
    "        else:\n",
    "            step_time = step_end_time - start_time\n",
    "\n",
    "        total_time_spent = step_end_time - start_time\n",
    "        avg_step_time = total_time_spent / (step + 1)\n",
    "        etc = avg_step_time * (num_steps - (step + 1))\n",
    "\n",
    "        print(\n",
    "            f\"Step [{step + 1}/{num_steps}], Loss: {loss.item():.4f}, Step Time: {step_time:.4f}s, ETC: {etc/3600:.2f} hours\"\n",
    "        )\n",
    "        wandb.log({\n",
    "            \"Training Loss\": loss.item(), \n",
    "            \"Step\": step + 1, \n",
    "            \"Step Time\": step_time, \n",
    "            \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "            \"ETC (hours)\": etc / 3600\n",
    "        })\n",
    "\n",
    "        if (step + 1) % validation_steps == 0:\n",
    "            validate_model(model, test_dataloader, criterion)\n",
    "\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f'checkpoint.pth')\n",
    "\n",
    "    avg_train_loss = running_loss / num_steps\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Average Training Loss: {avg_train_loss:.4f}, Total Time: {total_time:.4f}s\"\n",
    "    )\n",
    "    wandb.log({\"Average Training Loss\": avg_train_loss, \"Total Time\": total_time})\n",
    "\n",
    "    validate_model(model, test_dataloader, criterion)\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training!\n",
    "\n",
    "All thats left now is to train the model! Live stats can be viewed on the wandb page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    wandb.init(project=\"convnet-colorizer\")\n",
    "\n",
    "    wandb.config.update(asdict(settings))\n",
    "    wandb.run.name = settings.create_run_name()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Total images: {settings.total_image_count}\")\n",
    "    print(f\"Total val images: {settings.validation_image_count}\")\n",
    "\n",
    "    dataset = ImageFolderRGBDataset(root_dir=settings.root_dir, transform=None)\n",
    "    train_size = settings.total_image_count - settings.validation_image_count\n",
    "    test_size = settings.validation_image_count\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=settings.batch_size, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=settings.batch_size, shuffle=False, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = ConvNet().to(\"cuda\")\n",
    "    wandb.watch(model)\n",
    "\n",
    "    criterion = getattr(nn, settings.loss_function)()\n",
    "    optimizer = getattr(optim, settings.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr=settings.learning_rate,\n",
    "        weight_decay=settings.weight_decay,\n",
    "    )\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        settings.num_steps,\n",
    "        settings.validation_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
