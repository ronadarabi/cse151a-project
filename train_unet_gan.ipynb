{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet GAN Finetuning\n",
    "\n",
    "## Important note:\n",
    "**Multithreading does not work in jupyter notebooks, so its disabled here. This file is meant as an overview of the training process, it works but since theres no concurrency it takes aproximately 4 times as long to train**\n",
    "\n",
    "This notebook can be used to follow the finetunings steps of the unet for our final model, in the `training_scripts` subfolder of this repository you can find the multithreaded version of this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you haven't installed the required libraries\n",
    "#!pip install -q wandb torch torchvision numpy<2.0.0 pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This cell contains utilities for converting from RGB to LAB colorspace using torch to speed up operations at training and inference time. The code and conversions were taken from skimage and modified to work with torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def rgb_to_lab(rgb_image: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    with torch.no_grad():\n",
    "        RGB_TO_XYZ = torch.tensor(\n",
    "            [\n",
    "                [0.412453, 0.357580, 0.180423],\n",
    "                [0.212671, 0.715160, 0.072169],\n",
    "                [0.019334, 0.119193, 0.950227],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=rgb_image.device,\n",
    "        )\n",
    "        XYZ_REF = torch.tensor(\n",
    "            [0.95047, 1.0, 1.08883], dtype=torch.float32, device=rgb_image.device\n",
    "        )\n",
    "\n",
    "        # Convert RGB to linear RGB\n",
    "        if rgb_image.max() > 1.0:\n",
    "            rgb_image = rgb_image / 255.0\n",
    "\n",
    "        mask = rgb_image > 0.04045\n",
    "        rgb_linear = torch.where(\n",
    "            mask, torch.pow(((rgb_image + 0.055) / 1.055), 2.4), rgb_image / 12.92\n",
    "        )\n",
    "\n",
    "        # Convert linear RGB to XYZ\n",
    "        xyz = torch.matmul(rgb_linear, RGB_TO_XYZ.t())\n",
    "\n",
    "        # Normalize XYZ values\n",
    "        xyz_scaled = xyz / XYZ_REF\n",
    "\n",
    "        # XYZ to LAB conversion\n",
    "        epsilon = 0.008856\n",
    "        kappa = 903.3\n",
    "\n",
    "        f = torch.where(\n",
    "            xyz > epsilon, xyz_scaled.pow(1 / 3), (kappa * xyz_scaled + 16) / 116\n",
    "        )\n",
    "\n",
    "        x, y, z = f[..., 0], f[..., 1], f[..., 2]\n",
    "        l = (116 * y - 16).unsqueeze(0)\n",
    "        a = (500 * (x - y)).unsqueeze(0)\n",
    "        b = (200 * (y - z)).unsqueeze(0)\n",
    "        ab = torch.cat([a, b], dim=0)\n",
    "\n",
    "        # Normalize to [-1, 1]\n",
    "        l = (l / 50.0) - 1.0\n",
    "        ab = ab / 110.0\n",
    "\n",
    "        return l, ab\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def lab_to_rgb(l: torch.Tensor, ab: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        XYZ_TO_RGB = torch.tensor(\n",
    "            [\n",
    "                [3.24048134, -1.53715152, -0.49853633],\n",
    "                [-0.96925495, 1.87599, 0.04155593],\n",
    "                [0.05564664, -0.20404134, 1.05731107],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=l.device,\n",
    "        )\n",
    "        XYZ_REF = torch.tensor(\n",
    "            [0.95047, 1.0, 1.08883], dtype=torch.float32, device=l.device\n",
    "        ).view(1, 3, 1, 1)\n",
    "\n",
    "        if l.dim() == 3:\n",
    "            l = l.unsqueeze(0)\n",
    "        if ab.dim() == 3:\n",
    "            ab = ab.unsqueeze(0)\n",
    "\n",
    "        # Denormalize from [-1, 1]\n",
    "        l = (l + 1.0) * 50.0\n",
    "        ab = ab * 110.0\n",
    "\n",
    "        y = (l + 16) / 116\n",
    "        x = ab[:, 0:1] / 500 + y\n",
    "        z = y - ab[:, 1:2] / 200\n",
    "\n",
    "        xyz = torch.cat([x, y, z], dim=1)\n",
    "\n",
    "        mask = xyz > 0.2068966\n",
    "        xyz = torch.where(mask, xyz.pow(3), (xyz - 16 / 116) / 7.787)\n",
    "\n",
    "        xyz = xyz * XYZ_REF\n",
    "\n",
    "        batch_size, _, height, width = xyz.shape\n",
    "        xyz_reshaped = xyz.view(batch_size, 3, -1)\n",
    "\n",
    "        rgb_linear = torch.bmm(XYZ_TO_RGB.expand(batch_size, -1, -1), xyz_reshaped)\n",
    "\n",
    "        rgb_linear = rgb_linear.view(batch_size, 3, height, width)\n",
    "\n",
    "        mask = rgb_linear > 0.0031308\n",
    "        rgb = torch.where(\n",
    "            mask, 1.055 * rgb_linear.pow(1 / 2.4) - 0.055, 12.92 * rgb_linear\n",
    "        )\n",
    "\n",
    "        return rgb.clamp(0, 1).permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith((\".jpg\")):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = torch.from_numpy(np.array(image).astype(np.float32))\n",
    "        l_chan, ab_chan = rgb_to_lab(image)\n",
    "        return l_chan, ab_chan\n",
    "\n",
    "\n",
    "class FastColorizationDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, num_workers, pin_memory):\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            # Multiprocessing is not supported in Jupyter Notebooks, so we disable it\n",
    "            #num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "        self.stream = torch.cuda.Stream()\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = super().__iter__()\n",
    "        for data in iterator:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                yield [item.cuda(non_blocking=True) for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSettings:\n",
    "    finetune: bool = True\n",
    "    pretrained_path: str = \"model_unet_final.pth\"\n",
    "    root_dir: str = \"img_data\"\n",
    "    l1_loss_weight: float = 100.0\n",
    "    gan_loss_weight: float = 1.0\n",
    "    generator_lr: float = 2e-4\n",
    "    discriminator_lr: float = 2e-4\n",
    "    beta1: float = 0.5\n",
    "    beta2: float = 0.999\n",
    "    validation_image_count: int = 12\n",
    "    batch_size: int = 32\n",
    "    warmup_steps: int = 1000\n",
    "    validation_steps: int = 1000\n",
    "    model_name: str = \"Unet-GAN\"\n",
    "    gan_mode: str = \"vanilla\"\n",
    "\n",
    "    def set_total_image_count(self, count):\n",
    "        self.total_image_count = count\n",
    "        self.num_steps = (self.total_image_count - self.validation_image_count) // self.batch_size\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.total_image_count = None\n",
    "        self.num_steps = None\n",
    "\n",
    "    def create_run_name(self):\n",
    "        if self.finetune:\n",
    "            return f\"{self.model_name}_pretrained_bs{self.batch_size}_steps{self.num_steps}\"\n",
    "        return f\"{self.model_name}_bs{self.batch_size}_steps{self.num_steps}\"\n",
    "\n",
    "settings = ModelSettings()\n",
    "dataset = ColorizationDataset(root_dir=settings.root_dir)\n",
    "settings.set_total_image_count(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet (Generator)\n",
    "\n",
    "This is the unet model used in pretraining. We're going to plug it in to the GAN finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrissCrossAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.channel_out = in_dim // 8\n",
    "        # Combined QKV projection\n",
    "        self.qkv_conv = nn.Conv2d(in_dim, 3 * self.channel_out, 1)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.Conv2d(in_dim // 8, in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Combined QKV projection\n",
    "        qkv = self.qkv_conv(x)\n",
    "        query, key, value = qkv.chunk(3, dim=1)\n",
    "\n",
    "        # Horizontal attention\n",
    "        query_h = (\n",
    "            query.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        )\n",
    "        key_h = key.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        value_h = (\n",
    "            value.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        )\n",
    "\n",
    "        energy_h = torch.bmm(query_h, key_h.transpose(1, 2))\n",
    "        attn_h = F.softmax(energy_h, dim=-1)\n",
    "        out_h = torch.bmm(attn_h, value_h)\n",
    "\n",
    "        # Vertical attention\n",
    "        query_v = (\n",
    "            query.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        )\n",
    "        key_v = key.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        value_v = (\n",
    "            value.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        )\n",
    "\n",
    "        energy_v = torch.bmm(query_v, key_v.transpose(1, 2))\n",
    "        attn_v = F.softmax(energy_v, dim=-1)\n",
    "        out_v = torch.bmm(attn_v, value_v)\n",
    "\n",
    "        # Reshape and combine\n",
    "        out_h = out_h.view(B, W, self.channel_out, H).permute(0, 2, 3, 1)\n",
    "        out_v = out_v.view(B, H, self.channel_out, W).permute(0, 2, 1, 3)\n",
    "\n",
    "        out = out_h + out_v\n",
    "        out = self.gamma * out\n",
    "\n",
    "        # Project back to the original channel dimension\n",
    "        out = self.out_conv(out)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nf,\n",
    "        ni,\n",
    "        submodule=None,\n",
    "        input_c=None,\n",
    "        dropout=False,\n",
    "        innermost=False,\n",
    "        outermost=False,\n",
    "        use_attention=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        self.use_attention = use_attention\n",
    "        if input_c is None:\n",
    "            input_c = nf\n",
    "        downconv = nn.Conv2d(\n",
    "            input_c, ni, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4, stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni, nf, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni * 2, nf, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout:\n",
    "                up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        # Apply attention module if needed, except for the outermost layer and when the number of filters is too high\n",
    "        self.use_attention_this_layer = not outermost and nf <= 512 and use_attention\n",
    "        if self.use_attention_this_layer:\n",
    "            self.attention = CrissCrossAttention(nf)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            output = self.model(x)\n",
    "            if self.use_attention_this_layer:\n",
    "                output = self.attention(output)\n",
    "            return torch.cat([x, output], 1)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_c=1,\n",
    "        output_c=2,\n",
    "        n_down=8,\n",
    "        num_filters=64,\n",
    "        use_attention=True,\n",
    "        use_dropout=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(\n",
    "            num_filters * 8,\n",
    "            num_filters * 8,\n",
    "            innermost=True,\n",
    "            use_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(\n",
    "                num_filters * 8,\n",
    "                num_filters * 8,\n",
    "                submodule=unet_block,\n",
    "                dropout=use_dropout,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(\n",
    "                out_filters // 2,\n",
    "                out_filters,\n",
    "                submodule=unet_block,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "            out_filters //= 2\n",
    "\n",
    "        self.model = UnetBlock(\n",
    "            output_c,\n",
    "            out_filters,\n",
    "            input_c=input_c,\n",
    "            submodule=unet_block,\n",
    "            outermost=True,\n",
    "            use_attention=use_attention,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "\n",
    "This discriminator architechure was found to be the most effective at producing favorable images, it analyzes the input images at multiple scales and predicts whether the given image is 'fake' or 'real'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name=\"weight\", power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))\n",
    "\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = nn.Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_layers=5):\n",
    "        super().__init__()\n",
    "        layers = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        for i in range(1, n_layers):\n",
    "            nf_prev = num_filters * min(2 ** (i - 1), 8)\n",
    "            nf = num_filters * min(2**i, 8)\n",
    "            stride = 1 if i == n_layers - 1 else 2\n",
    "            layers.append(self.get_layers(nf_prev, nf, s=stride))\n",
    "        layers.append(self.get_layers(nf, 1, s=1, norm=False, act=False))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True):\n",
    "        layers = [SpectralNorm(nn.Conv2d(ni, nf, k, s, p, bias=not norm))]\n",
    "        if norm:\n",
    "            layers += [nn.InstanceNorm2d(nf)]\n",
    "        if act:\n",
    "            layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_layers=5, num_D=3):\n",
    "        super().__init__()\n",
    "        self.num_D = num_D\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        for i in range(num_D):\n",
    "            netD = Discriminator(input_c, num_filters, n_layers)\n",
    "            setattr(self, f\"layer_{i}\", netD)\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(\n",
    "            3, stride=2, padding=[1, 1], count_include_pad=False\n",
    "        )\n",
    "\n",
    "    def singleD_forward(self, model, input):\n",
    "        result = [input]\n",
    "        for i in range(len(model)):\n",
    "            result.append(model[i](result[-1]))\n",
    "        return result[1:]\n",
    "\n",
    "    def forward(self, input):\n",
    "        result = []\n",
    "        input_downsampled = input\n",
    "        for i in range(self.num_D):\n",
    "            model = getattr(self, f\"layer_{i}\")\n",
    "            result.append(self.singleD_forward(model.model, input_downsampled))\n",
    "            if i != (self.num_D - 1):\n",
    "                input_downsampled = self.downsample(input_downsampled)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss\n",
    "\n",
    "The gan loss takes the logits of discrimator and produces a loss that the generator can use improve its image outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode=\"vanilla\", real_label=0.9, fake_label=0.1):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"real_label\", torch.tensor(real_label))\n",
    "        self.register_buffer(\"fake_label\", torch.tensor(fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == \"vanilla\":\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == \"lsgan\":\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"gan_mode {gan_mode} not implemented\")\n",
    "\n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        target_tensor = self.get_labels(prediction, target_is_real)\n",
    "        loss = self.loss(prediction, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main GAN Model\n",
    "\n",
    "An amalgomation of the above networks that puts the generator and discriminator into a zero sum game. The generator attempts to generate images that fool the discriminator, and the discriminator attempts to classify images from the generator as 'fake'. The result of this game is a (hopefully) more effective generator model that produces higher quality colorizations, however the gan architechure will not ever converge to a final solution and can sometimes suffer mode collapse. Because of this each `n` steps are logged and images of its output are saved so that it can be later decided which iteration of the generator model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net_G=None,\n",
    "        lr_G=1e-4,\n",
    "        lr_D=4e-4,\n",
    "        beta1=0.5,\n",
    "        beta2=0.999,\n",
    "        lambda_L1=100.0,\n",
    "        lambda_GAN=1.0,\n",
    "        gan_mode=\"vanilla\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        self.lambda_GAN = lambda_GAN\n",
    "\n",
    "        if net_G is None:\n",
    "            self.net_G = Unet(input_c=1, output_c=2, n_down=8, num_filters=64).to(self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "\n",
    "        self.net_D = MultiScaleDiscriminator(input_c=3, num_filters=64, n_layers=5, num_D=3).to(self.device)\n",
    "        self.GANcriterion = GANLoss(gan_mode=gan_mode).to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "\n",
    "        self.loss_G_GAN = None\n",
    "        self.loss_G_L1 = None\n",
    "        self.loss_D = None\n",
    "        self.loss_D_real = None\n",
    "        self.loss_D_fake = None\n",
    "\n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "    def setup_input(self, l_chan, ab_chan):\n",
    "        self.l_chan = l_chan.to(self.device)\n",
    "        self.ab_chan = ab_chan.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.l_chan)\n",
    "\n",
    "    def backward_D(self):\n",
    "        fake_image = torch.cat([self.l_chan, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.l_chan, self.ab_chan], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.l_chan, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True) * self.lambda_GAN\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab_chan) * self.lambda_L1\n",
    "        \n",
    "        real_rgb = lab_to_rgb(self.l_chan, self.ab_chan)\n",
    "        fake_rgb = lab_to_rgb(self.l_chan, self.fake_color)\n",
    "        \n",
    "        real_rgb = real_rgb.permute(0, 3, 1, 2)\n",
    "        fake_rgb = fake_rgb.permute(0, 3, 1, 2)\n",
    "        \n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()\n",
    "    \n",
    "    def compute_D_loss(self):\n",
    "        real_image = torch.cat([self.l_chan, self.ab_chan], dim=1)\n",
    "        fake_image = torch.cat([self.l_chan, self.fake_color.detach()], dim=1)\n",
    "        \n",
    "        real_preds = self.net_D(real_image)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        \n",
    "        self.loss_D_real = 0\n",
    "        self.loss_D_fake = 0\n",
    "        for real_scale_preds, fake_scale_preds in zip(real_preds, fake_preds):\n",
    "            self.loss_D_real += self.GANcriterion(real_scale_preds[-1], True)\n",
    "            self.loss_D_fake += self.GANcriterion(fake_scale_preds[-1], False)\n",
    "        \n",
    "        self.loss_D_real /= len(real_preds)\n",
    "        self.loss_D_fake /= len(fake_preds)\n",
    "        self.loss_D = (self.loss_D_real + self.loss_D_fake) * 0.5\n",
    "        \n",
    "        return self.loss_D\n",
    "\n",
    "    def compute_G_loss(self):\n",
    "        fake_image = torch.cat([self.l_chan, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        \n",
    "        self.loss_G_GAN = 0\n",
    "        for scale_preds in fake_preds:\n",
    "            self.loss_G_GAN += self.GANcriterion(scale_preds[-1], True)\n",
    "        self.loss_G_GAN /= len(fake_preds)\n",
    "        \n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab_chan) * self.lambda_L1\n",
    "        loss_G = self.loss_G_GAN * self.lambda_GAN + self.loss_G_L1\n",
    "        return loss_G\n",
    "\n",
    "    def train(self):\n",
    "        self.net_G.train()\n",
    "        self.net_D.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.net_G.eval()\n",
    "        self.net_D.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, test_dataloader):\n",
    "    torch.cuda.synchronize()\n",
    "    val_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss_G = 0.0\n",
    "    val_loss_D = 0.0\n",
    "    total_images = 0\n",
    "    logged_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for l_chan, ab_chan in test_dataloader:\n",
    "            l_chan, ab_chan = l_chan.to(device), ab_chan.to(device)\n",
    "\n",
    "            model.setup_input(l_chan, ab_chan)\n",
    "            model.forward()\n",
    "\n",
    "            loss_D = model.compute_D_loss()\n",
    "            loss_G = model.compute_G_loss()\n",
    "\n",
    "            val_loss_G += loss_G.item()\n",
    "            val_loss_D += loss_D.item()\n",
    "            total_images += l_chan.size(0)\n",
    "\n",
    "            if logged_images < settings.display_imgs:\n",
    "                num_samples = min(\n",
    "                    settings.display_imgs - logged_images, l_chan.shape[0]\n",
    "                )\n",
    "                l_chan_samples = l_chan[:num_samples]\n",
    "                output_samples = model.fake_color[:num_samples]\n",
    "                target_samples = ab_chan[:num_samples]\n",
    "\n",
    "                greyscale_samples = lab_to_rgb(\n",
    "                    l_chan_samples, torch.zeros_like(output_samples)\n",
    "                )\n",
    "                output_rgb_samples = lab_to_rgb(l_chan_samples, output_samples)\n",
    "                target_rgb_samples = lab_to_rgb(l_chan_samples, target_samples)\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"Examples\": wandb.Image(\n",
    "                            np.vstack(\n",
    "                                [\n",
    "                                    np.hstack(greyscale_samples.cpu().numpy()),\n",
    "                                    np.hstack(output_rgb_samples.cpu().numpy()),\n",
    "                                    np.hstack(target_rgb_samples.cpu().numpy()),\n",
    "                                ]\n",
    "                            ),\n",
    "                            caption=\"Top: Grayscale, Middle: Predicted, Bottom: True\",\n",
    "                        )\n",
    "                    },\n",
    "                    commit=False,\n",
    "                )\n",
    "\n",
    "                logged_images += num_samples\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    avg_val_loss_G = val_loss_G / (total_images / settings.batch_size)\n",
    "    avg_val_loss_D = val_loss_D / (total_images / settings.batch_size)\n",
    "    val_time = time.time() - val_start_time\n",
    "    print(\n",
    "        f\"Average Validation Loss G: {avg_val_loss_G:.4f}, D: {avg_val_loss_D:.4f}, Validation Time: {val_time:.4f}s\"\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"Validation Loss G\": avg_val_loss_G,\n",
    "            \"Validation Loss D\": avg_val_loss_D,\n",
    "            \"Validation Time\": val_time,\n",
    "        },\n",
    "        commit=False,\n",
    "    )\n",
    "    return avg_val_loss_G + avg_val_loss_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    num_steps,\n",
    "    validation_steps,\n",
    "):\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    step_times = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    train_iter = iter(train_dataloader)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        try:\n",
    "            l_chan, ab_chan = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_dataloader)\n",
    "            l_chan, ab_chan = next(train_iter)\n",
    "\n",
    "        l_chan, ab_chan = l_chan.to(device), ab_chan.to(device)\n",
    "\n",
    "        # Update Discriminator\n",
    "        model.set_requires_grad(model.net_D, True)\n",
    "        model.opt_D.zero_grad(set_to_none=True)\n",
    "        model.setup_input(l_chan, ab_chan)\n",
    "        model.forward()\n",
    "        loss_D = model.compute_D_loss()\n",
    "        loss_D.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.net_D.parameters(), max_norm=1.0)\n",
    "        model.opt_D.step()\n",
    "\n",
    "        # Update Generator\n",
    "        model.set_requires_grad(model.net_D, False)\n",
    "        model.opt_G.zero_grad(set_to_none=True)\n",
    "        loss_G = model.compute_G_loss()\n",
    "        loss_G.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.net_G.parameters(), max_norm=1.0)\n",
    "        model.opt_G.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        step_end_time = time.time()\n",
    "        step_times.append(step_end_time)\n",
    "\n",
    "        if step > 0:\n",
    "            step_time = step_times[-1] - step_times[-2]\n",
    "        else:\n",
    "            step_time = step_end_time - start_time\n",
    "\n",
    "        total_time_spent = step_end_time - start_time\n",
    "        avg_step_time = total_time_spent / (step + 1)\n",
    "        etc_seconds = avg_step_time * (num_steps - (step + 1))\n",
    "        etc_hours = int(etc_seconds // 3600)\n",
    "        etc_minutes = int((etc_seconds % 3600) // 60)\n",
    "        time_to_next_checkpoint = avg_step_time * (\n",
    "            validation_steps - (step % validation_steps)\n",
    "        )\n",
    "\n",
    "        if model.loss_G_GAN is not None:\n",
    "            loss_G_GAN = model.loss_G_GAN.item()\n",
    "        else:\n",
    "            loss_G_GAN = 0.0\n",
    "\n",
    "        if model.loss_G_L1 is not None:\n",
    "            loss_G_L1 = model.loss_G_L1.item()\n",
    "        else:\n",
    "            loss_G_L1 = 0.0\n",
    "\n",
    "        print(\n",
    "            f\"Step [{step + 1}/{num_steps}], \"\n",
    "            f\"Loss G: {loss_G.item():.4f}, \"\n",
    "            f\"Loss D: {loss_D.item():.4f}, \"\n",
    "            f\"Step Time: {step_time:.4f}s, \"\n",
    "            f\"ETC: {etc_hours}h {etc_minutes}m, \"\n",
    "            f\"Next Checkpoint: {time_to_next_checkpoint/60:.2f} minutes\"\n",
    "        )\n",
    "\n",
    "        log_dict = {\n",
    "            \"Generator Loss\": loss_G,\n",
    "            \"Generator Loss GAN\": loss_G_GAN,\n",
    "            \"Generator Loss L1\": loss_G_L1,\n",
    "            \"Discriminator Loss\": loss_D,\n",
    "            \"Discriminator Loss Real\": model.loss_D_real.item(),\n",
    "            \"Discriminator Loss Fake\": model.loss_D_fake.item(),\n",
    "            \"Step\": step + 1,\n",
    "            \"Step Time\": step_time,\n",
    "            \"ETC (hours)\": etc_seconds / 3600,\n",
    "            \"Time to Next Checkpoint (minutes)\": time_to_next_checkpoint / 60,\n",
    "        }\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "        if (step + 1) % validation_steps == 0:\n",
    "            model.eval()\n",
    "            val_loss = validate_model(model, test_dataloader)\n",
    "            model.train()\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"step\": step + 1,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_G_state_dict\": model.opt_G.state_dict(),\n",
    "                        \"optimizer_D_state_dict\": model.opt_D.state_dict(),\n",
    "                        \"loss_G\": loss_G.item(),\n",
    "                        \"loss_D\": loss_D.item(),\n",
    "                    },\n",
    "                    f\"best_checkpoint_unet_gan.pth\",\n",
    "                )\n",
    "                \n",
    "            torch.save(\n",
    "                {\n",
    "                    \"step\": step + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_G_state_dict\": model.opt_G.state_dict(),\n",
    "                    \"optimizer_D_state_dict\": model.opt_D.state_dict(),\n",
    "                    \"loss_G\": loss_G.item(),\n",
    "                    \"loss_D\": loss_D.item(),\n",
    "                },\n",
    "                f\"checkpoint_unet_gan_{step + 1}.pth\",\n",
    "            )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Total Time: {total_time:.4f}s\")\n",
    "    wandb.log({\"Total Time\": total_time})\n",
    "\n",
    "    validate_model(model, test_dataloader)\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_final_unet_gan.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #multiprocessing.freeze_support()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    wandb.init(project=\"unet-gan-colorizer\")\n",
    "\n",
    "    wandb.config.update(asdict(settings))\n",
    "    wandb.run.name = settings.create_run_name()\n",
    "\n",
    "    train_size = settings.total_image_count - settings.validation_image_count\n",
    "    test_size = settings.validation_image_count\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = FastColorizationDataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=settings.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = FastColorizationDataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=settings.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    pretrained_unet = Unet(input_c=1, output_c=2, n_down=8, num_filters=64)\n",
    "    if settings.finetune:\n",
    "        state_dict = torch.load(settings.pretrained_path)\n",
    "        if 'net_G' in state_dict:\n",
    "            # The model was saved as part of MainModel\n",
    "            pretrained_unet.load_state_dict({k.replace('net_G.', ''): v for k, v in state_dict['net_G'].items()})\n",
    "        elif 'model_state_dict' in state_dict:\n",
    "            # The model was saved with the entire state dict\n",
    "            pretrained_unet.load_state_dict({k.replace('net_G.', ''): v for k, v in state_dict['model_state_dict'].items() if k.startswith('net_G.')})\n",
    "        else:\n",
    "            # Try loading directly\n",
    "            pretrained_unet.load_state_dict({k.replace('net_G.', ''): v for k, v in state_dict.items() if k.startswith('net_G.')})\n",
    "        print(f\"Loaded pretrained model from {settings.pretrained_path}\")\n",
    "\n",
    "    model = MainModel(\n",
    "        net_G=pretrained_unet,\n",
    "        lr_G=settings.generator_lr,\n",
    "        lr_D=settings.discriminator_lr,\n",
    "        beta1=settings.beta1,\n",
    "        beta2=settings.beta2,\n",
    "        lambda_L1=settings.l1_loss_weight,\n",
    "        lambda_GAN=settings.gan_loss_weight,\n",
    "        gan_mode=settings.gan_mode,\n",
    "    ).to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    print(f\"Generator Parameters: {sum(p.numel() for p in model.net_G.parameters())}\")\n",
    "    print(f\"Discriminator Parameters: {sum(p.numel() for p in model.net_D.parameters())}\")\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        settings.num_steps,\n",
    "        settings.validation_steps,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
