{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Pretraining\n",
    "\n",
    "## Important note:\n",
    "**Multithreading does not work in jupyter notebooks, so its disabled here. This file is meant as an overview of the training process, it works but since theres no concurrency it takes aproximately 4 times as long to train**\n",
    "\n",
    "This notebook can be used to follow the pretraining steps of the base unet for our final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you haven't installed the required libraries\n",
    "#!pip install -q wandb torch torchvision numpy<2.0.0 pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This cell contains utilities for converting from RGB to LAB colorspace using torch to speed up operations at training and inference time. The code and conversions were taken from skimage and modified to work with torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def rgb_to_lab(rgb_image: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    with torch.no_grad():\n",
    "        RGB_TO_XYZ = torch.tensor(\n",
    "            [\n",
    "                [0.412453, 0.357580, 0.180423],\n",
    "                [0.212671, 0.715160, 0.072169],\n",
    "                [0.019334, 0.119193, 0.950227],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=rgb_image.device,\n",
    "        )\n",
    "        XYZ_REF = torch.tensor(\n",
    "            [0.95047, 1.0, 1.08883], dtype=torch.float32, device=rgb_image.device\n",
    "        )\n",
    "\n",
    "        # Convert RGB to linear RGB\n",
    "        if rgb_image.max() > 1.0:\n",
    "            rgb_image = rgb_image / 255.0\n",
    "\n",
    "        mask = rgb_image > 0.04045\n",
    "        rgb_linear = torch.where(\n",
    "            mask, torch.pow(((rgb_image + 0.055) / 1.055), 2.4), rgb_image / 12.92\n",
    "        )\n",
    "\n",
    "        # Convert linear RGB to XYZ\n",
    "        xyz = torch.matmul(rgb_linear, RGB_TO_XYZ.t())\n",
    "\n",
    "        # Normalize XYZ values\n",
    "        xyz_scaled = xyz / XYZ_REF\n",
    "\n",
    "        # XYZ to LAB conversion\n",
    "        epsilon = 0.008856\n",
    "        kappa = 903.3\n",
    "\n",
    "        f = torch.where(\n",
    "            xyz > epsilon, xyz_scaled.pow(1 / 3), (kappa * xyz_scaled + 16) / 116\n",
    "        )\n",
    "\n",
    "        x, y, z = f[..., 0], f[..., 1], f[..., 2]\n",
    "        l = (116 * y - 16).unsqueeze(0)\n",
    "        a = (500 * (x - y)).unsqueeze(0)\n",
    "        b = (200 * (y - z)).unsqueeze(0)\n",
    "        ab = torch.cat([a, b], dim=0)\n",
    "\n",
    "        # Normalize to [-1, 1]\n",
    "        l = (l / 50.0) - 1.0\n",
    "        ab = ab / 110.0\n",
    "\n",
    "        return l, ab\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def lab_to_rgb(l: torch.Tensor, ab: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        XYZ_TO_RGB = torch.tensor(\n",
    "            [\n",
    "                [3.24048134, -1.53715152, -0.49853633],\n",
    "                [-0.96925495, 1.87599, 0.04155593],\n",
    "                [0.05564664, -0.20404134, 1.05731107],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=l.device,\n",
    "        )\n",
    "        XYZ_REF = torch.tensor(\n",
    "            [0.95047, 1.0, 1.08883], dtype=torch.float32, device=l.device\n",
    "        ).view(1, 3, 1, 1)\n",
    "\n",
    "        if l.dim() == 3:\n",
    "            l = l.unsqueeze(0)\n",
    "        if ab.dim() == 3:\n",
    "            ab = ab.unsqueeze(0)\n",
    "\n",
    "        # Denormalize from [-1, 1]\n",
    "        l = (l + 1.0) * 50.0\n",
    "        ab = ab * 110.0\n",
    "\n",
    "        y = (l + 16) / 116\n",
    "        x = ab[:, 0:1] / 500 + y\n",
    "        z = y - ab[:, 1:2] / 200\n",
    "\n",
    "        xyz = torch.cat([x, y, z], dim=1)\n",
    "\n",
    "        mask = xyz > 0.2068966\n",
    "        xyz = torch.where(mask, xyz.pow(3), (xyz - 16 / 116) / 7.787)\n",
    "\n",
    "        xyz = xyz * XYZ_REF\n",
    "\n",
    "        batch_size, _, height, width = xyz.shape\n",
    "        xyz_reshaped = xyz.view(batch_size, 3, -1)\n",
    "\n",
    "        rgb_linear = torch.bmm(XYZ_TO_RGB.expand(batch_size, -1, -1), xyz_reshaped)\n",
    "\n",
    "        rgb_linear = rgb_linear.view(batch_size, 3, height, width)\n",
    "\n",
    "        mask = rgb_linear > 0.0031308\n",
    "        rgb = torch.where(\n",
    "            mask, 1.055 * rgb_linear.pow(1 / 2.4) - 0.055, 12.92 * rgb_linear\n",
    "        )\n",
    "\n",
    "        return rgb.clamp(0, 1).permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith((\".jpg\")):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = torch.from_numpy(np.array(image).astype(np.float32))\n",
    "        l_chan, ab_chan = rgb_to_lab(image)\n",
    "        return l_chan, ab_chan\n",
    "\n",
    "\n",
    "class FastColorizationDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, num_workers, pin_memory):\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            # Multiprocessing is not supported in Jupyter Notebooks, so we disable it\n",
    "            #num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "        self.stream = torch.cuda.Stream()\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = super().__iter__()\n",
    "        for data in iterator:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                yield [item.cuda(non_blocking=True) for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSettings:\n",
    "    finetune: bool = False\n",
    "    pretrained_model_path: str = \"\"\n",
    "    root_dir: str = \"img_data\"\n",
    "    finetune_learning_rate: float = 0.00035\n",
    "    validation_image_count: int = 1024\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 0.0007  # Peak LR\n",
    "    min_lr: float = learning_rate / 10  # Minimum LR\n",
    "    weight_decay: float = 1e-5\n",
    "    warmup_steps: int = 1000  # Linear warmup over n steps\n",
    "    validation_steps: int = 1000  # Validate every n steps\n",
    "    display_imgs: int = 12\n",
    "    early_stopping_patience: int = 5\n",
    "    loss_function: str = \"L1Loss\"\n",
    "    optimizer: str = \"AdamW\"\n",
    "    model_name: str = \"Unet\"\n",
    "\n",
    "    def set_total_image_count(self, count):\n",
    "        self.total_image_count = count\n",
    "        self.num_steps = (self.total_image_count - self.validation_image_count) // self.batch_size\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.total_image_count = None\n",
    "        self.num_steps = None\n",
    "\n",
    "    def create_run_name(self):\n",
    "        prefix = \"finetune_\" if self.finetune else \"\"\n",
    "        learning_rate = self.finetune_learning_rate if self.finetune else self.learning_rate\n",
    "        return f\"{prefix}{self.model_name}_lr{learning_rate}_bs{self.batch_size}_steps{self.num_steps}_loss{self.loss_function}_opt{self.optimizer}\"\n",
    "\n",
    "settings = ModelSettings()\n",
    "dataset = ColorizationDataset(root_dir=settings.root_dir)\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "settings.set_total_image_count(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet Model\n",
    "\n",
    "This is the unet model used for training, it can be initialized with or without Criss Cross self attention. The Criss Cross attention is a modification of the standard transformer attention that only attends to the pixels in the same row and column as the pixel in question, which drastically reduces the memory requirements. After two iterations of attention the model has at least indirectly attended to every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrissCrossAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.channel_out = in_dim // 8\n",
    "        # Combined QKV projection\n",
    "        self.qkv_conv = nn.Conv2d(in_dim, 3 * self.channel_out, 1)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.Conv2d(in_dim // 8, in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Combined QKV projection\n",
    "        qkv = self.qkv_conv(x)\n",
    "        query, key, value = qkv.chunk(3, dim=1)\n",
    "\n",
    "        # Horizontal attention\n",
    "        query_h = (\n",
    "            query.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        )\n",
    "        key_h = key.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        value_h = (\n",
    "            value.permute(0, 3, 1, 2).contiguous().view(B * W, self.channel_out, H)\n",
    "        )\n",
    "\n",
    "        energy_h = torch.bmm(query_h, key_h.transpose(1, 2))\n",
    "        attn_h = F.softmax(energy_h, dim=-1)\n",
    "        out_h = torch.bmm(attn_h, value_h)\n",
    "\n",
    "        # Vertical attention\n",
    "        query_v = (\n",
    "            query.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        )\n",
    "        key_v = key.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        value_v = (\n",
    "            value.permute(0, 2, 1, 3).contiguous().view(B * H, self.channel_out, W)\n",
    "        )\n",
    "\n",
    "        energy_v = torch.bmm(query_v, key_v.transpose(1, 2))\n",
    "        attn_v = F.softmax(energy_v, dim=-1)\n",
    "        out_v = torch.bmm(attn_v, value_v)\n",
    "\n",
    "        # Reshape and combine\n",
    "        out_h = out_h.view(B, W, self.channel_out, H).permute(0, 2, 3, 1)\n",
    "        out_v = out_v.view(B, H, self.channel_out, W).permute(0, 2, 1, 3)\n",
    "\n",
    "        out = out_h + out_v\n",
    "        out = self.gamma * out\n",
    "\n",
    "        # Project back to the original channel dimension\n",
    "        out = self.out_conv(out)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nf,\n",
    "        ni,\n",
    "        submodule=None,\n",
    "        input_c=None,\n",
    "        dropout=False,\n",
    "        innermost=False,\n",
    "        outermost=False,\n",
    "        use_attention=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        self.use_attention = use_attention\n",
    "        if input_c is None:\n",
    "            input_c = nf\n",
    "        downconv = nn.Conv2d(\n",
    "            input_c, ni, kernel_size=4, stride=2, padding=1, bias=False\n",
    "        )\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4, stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni, nf, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(\n",
    "                ni * 2, nf, kernel_size=4, stride=2, padding=1, bias=False\n",
    "            )\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout:\n",
    "                up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        # Apply attention module if needed, except for the outermost layer and when the number of filters is too high\n",
    "        self.use_attention_this_layer = not outermost and nf <= 512 and use_attention\n",
    "        if self.use_attention_this_layer:\n",
    "            self.attention = CrissCrossAttention(nf)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            output = self.model(x)\n",
    "            if self.use_attention_this_layer:\n",
    "                output = self.attention(output)\n",
    "            return torch.cat([x, output], 1)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_c=1,\n",
    "        output_c=2,\n",
    "        n_down=8,\n",
    "        num_filters=64,\n",
    "        use_attention=True,\n",
    "        use_dropout=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(\n",
    "            num_filters * 8,\n",
    "            num_filters * 8,\n",
    "            innermost=True,\n",
    "            use_attention=use_attention,\n",
    "        )\n",
    "\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(\n",
    "                num_filters * 8,\n",
    "                num_filters * 8,\n",
    "                submodule=unet_block,\n",
    "                dropout=use_dropout,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(\n",
    "                out_filters // 2,\n",
    "                out_filters,\n",
    "                submodule=unet_block,\n",
    "                use_attention=use_attention,\n",
    "            )\n",
    "            out_filters //= 2\n",
    "\n",
    "        self.model = UnetBlock(\n",
    "            output_c,\n",
    "            out_filters,\n",
    "            input_c=input_c,\n",
    "            submodule=unet_block,\n",
    "            outermost=True,\n",
    "            use_attention=use_attention,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learing Rate\n",
    "\n",
    "We used a custom learing rate schedule, a linear warmup followed by cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step: int):\n",
    "    if current_step < settings.warmup_steps:\n",
    "        return float(current_step) / float(max(1, settings.warmup_steps))\n",
    "    cosine_decay = 0.5 * (\n",
    "        1\n",
    "        + np.cos(\n",
    "            np.pi\n",
    "            * (current_step - settings.warmup_steps)\n",
    "            / (settings.num_steps - settings.warmup_steps)\n",
    "        )\n",
    "    )\n",
    "    return (\n",
    "        settings.min_lr + (settings.learning_rate - settings.min_lr) * cosine_decay\n",
    "    ) / settings.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation funtion\n",
    "\n",
    "If wandb is used this logs sample images. \n",
    "An example of the wandb log can be viewed [here](https://wandb.ai/danielpwarren/unet-colorizer/runs/9r7ls909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, test_dataloader, criterion):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    val_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_images = 0\n",
    "    logged_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for l_chan, ab_chan in test_dataloader:\n",
    "            with autocast():\n",
    "                outputs = model(l_chan)\n",
    "                loss = criterion(outputs, ab_chan)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            total_images += l_chan.size(0)\n",
    "\n",
    "            if logged_images < settings.display_imgs:\n",
    "                num_samples = min(settings.display_imgs - logged_images, l_chan.shape[0])\n",
    "                l_chan_samples = l_chan[:num_samples]\n",
    "                output_samples = outputs[:num_samples]\n",
    "                target_samples = ab_chan[:num_samples]\n",
    "\n",
    "                l_rgb_samples = lab_to_rgb(l_chan_samples, torch.zeros_like(output_samples))\n",
    "                output_rgb_samples = lab_to_rgb(l_chan_samples, output_samples)\n",
    "                target_rgb_samples = lab_to_rgb(l_chan_samples, target_samples)\n",
    "\n",
    "                l_rgb_samples = [sample.detach().cpu().numpy() for sample in l_rgb_samples]\n",
    "                output_rgb_samples = [sample.detach().cpu().numpy() for sample in output_rgb_samples]\n",
    "                target_rgb_samples = [sample.detach().cpu().numpy() for sample in target_rgb_samples]\n",
    "\n",
    "                stacked_L_rgb = np.hstack(l_rgb_samples)\n",
    "                stacked_output_rgb = np.hstack(output_rgb_samples)\n",
    "                stacked_target_rgb = np.hstack(target_rgb_samples)\n",
    "\n",
    "                stacked_images = np.vstack(\n",
    "                    (stacked_L_rgb, stacked_output_rgb, stacked_target_rgb)\n",
    "                )\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"Examples\": wandb.Image(\n",
    "                            stacked_images,\n",
    "                            caption=\"For each column: Top: Grayscale, Middle: Predicted, Bottom: True\",\n",
    "                        )\n",
    "                    },\n",
    "                    commit=False,\n",
    "                )\n",
    "\n",
    "                logged_images += l_chan.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / (total_images / settings.batch_size)\n",
    "    val_time = time.time() - val_start_time\n",
    "    print(\n",
    "        f\"Average Validation Loss: {avg_val_loss:.4f}, Validation Time: {val_time:.4f}s\"\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\"Average Validation Loss\": avg_val_loss, \"Validation Time\": val_time},\n",
    "        commit=False,\n",
    "    )\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "Calls validation function every `settings.validation_steps` steps, keeps track of validation improvement, and saves the model with the best validation loss. If there are `settings.early_stopping_patience` validation steps without improvement the model will stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_steps,\n",
    "    validation_steps,\n",
    "):\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step_times = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_iter = iter(train_dataloader)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        try:\n",
    "            l_chan, ab_chan = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_dataloader)\n",
    "            l_chan, ab_chan = next(train_iter)\n",
    "\n",
    "        l_chan, ab_chan = l_chan.to(\"cuda\"), ab_chan.to(\"cuda\")\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(l_chan)\n",
    "            loss = criterion(outputs, ab_chan)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        step_end_time = time.time()\n",
    "        step_times.append(step_end_time)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step > 0:\n",
    "            step_time = step_times[-1] - step_times[-2]\n",
    "        else:\n",
    "            step_time = step_end_time - start_time\n",
    "\n",
    "        total_time_spent = step_end_time - start_time\n",
    "        avg_step_time = total_time_spent / (step + 1)\n",
    "        etc_seconds = avg_step_time * (num_steps - (step + 1))\n",
    "        etc_hours = int(etc_seconds // 3600)\n",
    "        etc_minutes = int((etc_seconds % 3600) // 60)\n",
    "        time_to_next_checkpoint = avg_step_time * (\n",
    "            validation_steps - (step % validation_steps)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Step [{step + 1}/{num_steps}], \"\n",
    "            f\"Loss: {loss.item():.4f}, \"\n",
    "            f\"Step Time: {step_time:.4f}s, \"\n",
    "            f\"ETC: {etc_hours}h {etc_minutes}m, \"\n",
    "            f\"Next Checkpoint: {time_to_next_checkpoint/60:.2f} minutes\"\n",
    "        )\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"Training Loss\": loss.item(),\n",
    "                \"Step\": step + 1,\n",
    "                \"Step Time\": step_time,\n",
    "                \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "                \"ETC (hours)\": etc_seconds / 3600,\n",
    "                \"Time to Next Checkpoint (minutes)\": time_to_next_checkpoint / 60,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if (step + 1) % validation_steps == 0:\n",
    "            val_loss = validate_model(model, test_dataloader, criterion)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"step\": step + 1,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                        \"loss\": loss,\n",
    "                    },\n",
    "                    f\"best_checkpoint_unet.pth\",\n",
    "                )\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= settings.early_stopping_patience:\n",
    "                    print(\n",
    "                        f\"Early stopping triggered after {patience_counter} validations without improvement.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"step\": step + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                },\n",
    "                f\"checkpoint_unet.pth\",\n",
    "            )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    avg_train_loss = running_loss / num_steps\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}, Total Time: {total_time:.4f}s\")\n",
    "    wandb.log({\"Average Training Loss\": avg_train_loss, \"Total Time\": total_time})\n",
    "\n",
    "    validate_model(model, test_dataloader, criterion)\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_unet_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training\n",
    "\n",
    "All thats left now is to train the model! Live stats can be viewed on the wandb page if in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=\"unet-colorizer\")\n",
    "\n",
    "    wandb.config.update(asdict(settings))\n",
    "    wandb.run.name = settings.create_run_name()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Total images: {settings.total_image_count}\")\n",
    "    print(f\"Total val images: {settings.validation_image_count}\")\n",
    "\n",
    "    train_size = settings.total_image_count - settings.validation_image_count\n",
    "    test_size = settings.validation_image_count\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = FastColorizationDataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=settings.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = FastColorizationDataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=settings.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = Unet().to(device)\n",
    "\n",
    "    if settings.finetune:\n",
    "        print(f\"Loading pre-trained model from {settings.pretrained_model_path}\")\n",
    "        model.load_state_dict(torch.load(settings.pretrained_model_path))\n",
    "\n",
    "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "    wandb.watch(model)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    criterion = getattr(nn, settings.loss_function)()\n",
    "\n",
    "    if settings.finetune:\n",
    "        optimizer = getattr(optim, settings.optimizer)(\n",
    "            model.parameters(),\n",
    "            lr=settings.finetune_learning_rate,\n",
    "            weight_decay=settings.weight_decay,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = getattr(optim, settings.optimizer)(\n",
    "            model.parameters(),\n",
    "            lr=settings.learning_rate,\n",
    "            weight_decay=settings.weight_decay,\n",
    "        )\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        settings.num_steps,\n",
    "        settings.validation_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
